{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['a', 'b', 'c']\n",
    "lookup = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(\n",
    "                keys=tf.constant(keys),\n",
    "                values=tf.cast(tf.range(len(keys)), dtype=tf.int64),\n",
    "                key_dtype=tf.string,\n",
    "                value_dtype=tf.int64,\n",
    "            ),\n",
    "            default_value=-1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int64, numpy=\n",
       "array([[0, 1],\n",
       "       [1, 2]])>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup.lookup(tf.constant([[\"a\", \"b\"], [\"b\", \"c\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orbax.export import ExportManager\n",
    "from orbax.export import JaxModule\n",
    "from orbax.export import ServingConfig\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.training.train_state import TrainState\n",
    "import flax.linen as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        x, y, z = inputs\n",
    "        x = nn.Dense(10)(x)\n",
    "        y = nn.Dense(10)(y)\n",
    "        z = nn.Dense(10)(z) \n",
    "\n",
    "        c = jnp.concatenate([x, y, z], axis=-1)\n",
    "        out = nn.Dense(10)(c)\n",
    "        out = nn.relu(out)\n",
    "        out = nn.Dense(1)(out)\n",
    "        return out\n",
    "\n",
    "# init model\n",
    "rng = jax.random.PRNGKey(0)\n",
    "input_shape = (1, 2)\n",
    "model = FeedForward()\n",
    "params = model.init(rng, [jnp.ones(input_shape), jnp.ones(input_shape), jnp.ones(input_shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "tx = optax.sgd(lr, momentum)\n",
    "state = TrainState.create(apply_fn=model.apply, params=params[\"params\"], tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense_0': {'kernel': Array([[-1.0669333 , -0.8827529 , -0.00570564,  0.603013  ,  1.0019107 ,\n",
       "           0.28586274, -0.15108383, -0.7755998 , -0.7439601 , -0.6873577 ],\n",
       "         [-0.9427239 ,  1.2475272 ,  0.50599295,  1.0079471 ,  0.46328905,\n",
       "          -0.08936425, -0.39633456, -1.4424019 , -0.48588607,  0.4325949 ]],      dtype=float32),\n",
       "  'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)},\n",
       " 'Dense_1': {'kernel': Array([[-0.9376454 ,  1.3456881 ,  0.12843367,  0.04694894,  0.38935086,\n",
       "           0.16207337, -1.2485627 , -0.05399594,  0.6877025 , -0.91342294],\n",
       "         [ 0.39089563,  0.30140245, -0.18343109, -1.0258522 ,  0.26916406,\n",
       "           0.2980016 , -0.3358007 , -1.2467374 , -0.6431401 ,  1.0915588 ]],      dtype=float32),\n",
       "  'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)},\n",
       " 'Dense_2': {'kernel': Array([[ 0.6666576 , -1.0246173 , -0.05735719,  0.41652977,  1.2951125 ,\n",
       "          -0.07839004,  0.50164974, -0.95135075, -0.56714505,  0.06629798],\n",
       "         [ 1.4224155 , -0.20438029, -0.09926806,  1.3257306 , -0.30012968,\n",
       "           0.48361817, -0.58422804, -1.5310704 ,  0.6110954 , -1.0989282 ]],      dtype=float32),\n",
       "  'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)},\n",
       " 'Dense_3': {'kernel': Array([[-0.17999169, -0.18662179, -0.038037  ,  0.36720887, -0.27240565,\n",
       "           0.36469826, -0.1345222 ,  0.18939693,  0.22230588, -0.05519088],\n",
       "         [-0.01818199, -0.02700504, -0.14819585,  0.10009978, -0.02768263,\n",
       "          -0.16917312, -0.14676249, -0.13661501, -0.17948146, -0.02187661],\n",
       "         [ 0.04538824, -0.353176  , -0.11425097,  0.04738115,  0.13515063,\n",
       "          -0.09558514,  0.15530244,  0.05875126, -0.01177403, -0.13600108],\n",
       "         [-0.03664923,  0.0794163 , -0.2011252 , -0.2139023 , -0.39371815,\n",
       "          -0.18891518, -0.13209254,  0.16142426,  0.0866698 , -0.1644622 ],\n",
       "         [ 0.36898014,  0.03417228, -0.03441737, -0.27615854,  0.19162343,\n",
       "           0.161499  , -0.13048522,  0.2227989 , -0.16452597, -0.21884014],\n",
       "         [ 0.09890887, -0.11171573, -0.20537259, -0.14303333,  0.05188563,\n",
       "          -0.05284133,  0.24698704, -0.17165923, -0.041567  , -0.19365728],\n",
       "         [-0.20367086, -0.2513791 , -0.26761022, -0.04494913, -0.09066968,\n",
       "           0.25395975, -0.2513293 ,  0.16332133, -0.17146996,  0.01053473],\n",
       "         [-0.38947096, -0.2224506 , -0.00960024, -0.20391339, -0.06551483,\n",
       "           0.0540334 , -0.09317575, -0.11840244, -0.29438627, -0.10570519],\n",
       "         [ 0.01581431, -0.24894798, -0.2321265 ,  0.04207665, -0.29386136,\n",
       "           0.16713661, -0.35256195, -0.03190821, -0.10545468, -0.25486207],\n",
       "         [ 0.16460995, -0.21836257, -0.02313952,  0.00283183, -0.21456124,\n",
       "          -0.07824436,  0.37127787, -0.1113758 ,  0.03902167, -0.12632307],\n",
       "         [-0.18831767,  0.3856451 , -0.3681712 , -0.38456273,  0.13582356,\n",
       "           0.06743412, -0.37261242, -0.12663162,  0.06822988, -0.03458892],\n",
       "         [ 0.24666063, -0.16625921,  0.06775853,  0.04167796,  0.04974105,\n",
       "          -0.30958283, -0.07273696, -0.14258005,  0.01333679,  0.08643505],\n",
       "         [-0.02067972, -0.34176782,  0.16009772,  0.04734779,  0.14124238,\n",
       "           0.04675017,  0.20385318, -0.26395637,  0.10884858,  0.2918132 ],\n",
       "         [ 0.02890766,  0.14443943,  0.1365337 , -0.30674383,  0.11281377,\n",
       "          -0.06326106,  0.18632235,  0.2976141 ,  0.37865078,  0.00566028],\n",
       "         [-0.14314722,  0.02493831, -0.09277848, -0.037534  ,  0.1410771 ,\n",
       "          -0.00397234, -0.32446036,  0.0603639 , -0.14800869, -0.3532539 ],\n",
       "         [-0.09818688, -0.03062288, -0.03151249,  0.19313048,  0.05979627,\n",
       "           0.3710989 , -0.07986547,  0.04761159, -0.027485  , -0.06564774],\n",
       "         [-0.08382843,  0.2321462 ,  0.21837957, -0.02950148, -0.01883489,\n",
       "           0.13443267, -0.09942745,  0.36418   ,  0.05715526,  0.12927479],\n",
       "         [-0.11557156,  0.07710655, -0.19114208,  0.00189305, -0.05905517,\n",
       "           0.23866484, -0.17519844,  0.05663845, -0.28929204, -0.33098584],\n",
       "         [ 0.09952858, -0.04376971, -0.2532098 ,  0.08671062, -0.07232264,\n",
       "           0.05461558, -0.26486656, -0.10444976, -0.24116051,  0.19792598],\n",
       "         [ 0.03676486, -0.02289816,  0.01545098,  0.14841753, -0.08584208,\n",
       "           0.19700933,  0.04875481, -0.20468889,  0.01950783, -0.10681504],\n",
       "         [-0.30228436,  0.11566641,  0.21539266,  0.15052499,  0.06863248,\n",
       "           0.21028261,  0.06541496, -0.17319217,  0.21362413, -0.10874839],\n",
       "         [ 0.10434148,  0.23375028,  0.09613238, -0.02347801, -0.21573833,\n",
       "          -0.04946303,  0.12733243,  0.05466111, -0.18950757, -0.13244022],\n",
       "         [-0.15486927,  0.04511829, -0.03469037,  0.09343033, -0.24700847,\n",
       "          -0.06016932,  0.07290355, -0.2707753 , -0.17248031, -0.00249289],\n",
       "         [-0.14871834,  0.09109776, -0.15816016,  0.3107509 ,  0.18064624,\n",
       "          -0.06620125,  0.24888106,  0.32927367,  0.12114831,  0.14201745],\n",
       "         [-0.13340576, -0.07473632,  0.16697095,  0.09100944,  0.22733466,\n",
       "           0.10950468, -0.05190775,  0.09122618,  0.18814594,  0.04519042],\n",
       "         [ 0.20917328,  0.39810702,  0.22117124,  0.06520291, -0.00845009,\n",
       "           0.15721537,  0.40529945,  0.0682938 , -0.20668095,  0.32632908],\n",
       "         [ 0.17911208,  0.07399266, -0.13467488, -0.2784261 , -0.25896627,\n",
       "          -0.16638319, -0.18167846,  0.0578142 , -0.11029587,  0.00288512],\n",
       "         [ 0.02155744,  0.03190522,  0.01931643,  0.22763313,  0.1458976 ,\n",
       "           0.19356032,  0.34151885,  0.21785137,  0.07866407,  0.07420227],\n",
       "         [-0.04482054, -0.01443224,  0.08789773,  0.23950532, -0.15388255,\n",
       "           0.02045603,  0.0047655 ,  0.12391958, -0.03303996, -0.14202307],\n",
       "         [-0.23945878, -0.09824242,  0.17569298, -0.13161324, -0.00270537,\n",
       "          -0.1431382 , -0.260175  ,  0.02939474, -0.08833604,  0.07406711]],      dtype=float32),\n",
       "  'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)},\n",
       " 'Dense_4': {'kernel': Array([[-0.13772506],\n",
       "         [-0.44445324],\n",
       "         [ 0.12499338],\n",
       "         [ 0.18700336],\n",
       "         [-0.21347748],\n",
       "         [-0.22749935],\n",
       "         [ 0.34975165],\n",
       "         [ 0.24769351],\n",
       "         [ 0.11856047],\n",
       "         [ 0.36901763]], dtype=float32),\n",
       "  'bias': Array([0.], dtype=float32)}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.apply of FeedForward()>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.apply_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/example1_output_dir/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/example1_output_dir/assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "signature_dict = {\n",
    "  \"x\": tf.TensorSpec(shape=(2,), dtype=tf.float32), \n",
    "  \"y\": tf.TensorSpec(shape=(2,), dtype=tf.float32),\n",
    "  \"z\": tf.TensorSpec(shape=(2,), dtype=tf.float32)\n",
    "}\n",
    "@tf.function(input_signature=[signature_dict])\n",
    "def f(inputs):\n",
    "  return [inputs[\"x\"], inputs[\"y\"], inputs[\"z\"]]\n",
    "\n",
    "# Construct a JaxModule where JAX->TF conversion happens.\n",
    "jax_module = JaxModule({\"params\": state.params}, state.apply_fn)\n",
    "# Export the JaxModule along with one or more serving configs.\n",
    "export_mgr = ExportManager(\n",
    "  jax_module, [\n",
    "    ServingConfig(\n",
    "      'serving_default',\n",
    "      # input_signature=[tf.TensorSpec(shape=(10), dtype=tf.float32)],\n",
    "      tf_preprocessor=f,\n",
    "      # tf_postprocessor=example1_postprocess\n",
    "    ),\n",
    "])\n",
    "output_dir='/tmp/example1_output_dir'\n",
    "export_mgr.save(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.ones(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model output:  tf.Tensor([-0.12772207], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 19:46:38.041351: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "inputs = tf.random.normal([10,], dtype=tf.float32)\n",
    "batch = [inputs] * batch_size\n",
    "\n",
    "loaded_model = tf.saved_model.load(output_dir)\n",
    "loaded_model_outputs = loaded_model({\"x\": inputs, \"y\": inputs})\n",
    "print(\"loaded model output: \", loaded_model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x2c81d9af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x2c81d9af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for lookup_name in [\"album\", \"artist\", \"track\"]:\n",
    "    lookup = tf.keras.layers.StringLookup()\n",
    "    lookup.adapt(keys)\n",
    "    dst = f\"/tmp/vocab/lookup/{lookup_name}\"\n",
    "    if not tf.io.gfile.exists(dst):\n",
    "        tf.io.gfile.makedirs(dst)\n",
    "    lookup.save_assets(f\"/tmp/vocab/lookup/{lookup_name}\")\n",
    "\n",
    "tf.keras.layers.StringLookup().load_assets(\"/tmp/vocab/lookup/album/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_lookup(filename):\n",
    "    with tf.init_scope():\n",
    "        initializer = tf.lookup.TextFileInitializer(\n",
    "            filename,\n",
    "            key_dtype=tf.string, \n",
    "            key_index=tf.lookup.TextFileIndex.WHOLE_LINE, \n",
    "            value_dtype=tf.int64, \n",
    "            value_index=tf.lookup.TextFileIndex.LINE_NUMBER,\n",
    "            value_index_offset=1, # starting from 1\n",
    "        )\n",
    "        table = tf.lookup.StaticHashTable(initializer, 0)\n",
    "        \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
    "t = create_file_lookup(\"/tmp/vocab/lookup/album/vocabulary.txt\")\n",
    "ll = tf.keras.layers.Lambda(lambda x: t.lookup(x))\n",
    "ll.build((None, 1))\n",
    "m = tf.keras.Model()\n",
    "m.lookup = ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = create_file_lookup(\"/tmp/vocab/lookup/album/vocabulary.txt\")\n",
    "\n",
    "# .lookup(tf.constant([\"a\", \"b\", \"c\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = tf.keras.Sequential([\n",
    "    tf.keras.layers.StringLookup(\n",
    "        vocabulary=t,\n",
    "        mask_token=None,\n",
    "        num_oov_indices=0,\n",
    "        output_mode=\"int\",\n",
    "    )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.Sequential([tf.keras.layers.StringLookup(\n",
    "    vocabulary=[\"a\", \"b\", \"c\"],\n",
    "    mask_token=None,\n",
    "    num_oov_indices=0,\n",
    "    output_mode=\"int\",\n",
    ")])\n",
    "\n",
    "m.compile()\n",
    "\n",
    "m.predict([\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "t = create_file_lookup(\"/tmp/vocab/lookup/album/vocabulary.txt\")\n",
    "t.lookup(tf.constant([\"a\", \"b\", \"c\"]))\n",
    "l = tf.keras.layers.StringLookup(\n",
    "    vocabulary=[\"a\", \"b\", \"c\"],\n",
    "    mask_token=None,\n",
    "    num_oov_indices=0,\n",
    "    output_mode=\"int\",\n",
    ")(inputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "m = tf.keras.Model(\n",
    "    inputs=inputs,\n",
    "    outputs=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.Model()\n",
    "m.lookup = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.engine.training.Model object at 0x2cca173d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.engine.training.Model object at 0x2cca173d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/vocab/lookup/album/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/vocab/lookup/album/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.saved_model.save(m, \"/tmp/vocab/lookup/album/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(\"/tmp/vocab/lookup/album/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[0]])>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded(tf.constant([[\"a\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x2cc4eff10>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.Sequential([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_vocab(src):\n",
    "    lookup_layer = tf.keras.layers.StringLookup()\n",
    "    lookup_layer.load_assets(src)\n",
    "\n",
    "    return lookup_layer.get_vocabulary()\n",
    "\n",
    "\n",
    "    \n",
    "def make_servable(model, src, dst):\n",
    "    vocab = get_vocab(src)\n",
    "\n",
    "    lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(\n",
    "            keys=tf.constant(vocab),\n",
    "            values=tf.cast(tf.range(len(vocab)), dtype=tf.int64),\n",
    "            key_dtype=tf.string,\n",
    "            value_dtype=tf.int64,\n",
    "        ),\n",
    "        num_oov_buckets=1,\n",
    "    )\n",
    "\n",
    "    model.lookup = lookup_table\n",
    "\n",
    "\n",
    "signature_dict = {\n",
    "\"album\": tf.TensorSpec(shape=(2), dtype=tf.string), \n",
    "\"artist\": tf.TensorSpec(shape=(2), dtype=tf.string),\n",
    "\"track\": tf.TensorSpec(shape=(2), dtype=tf.string)\n",
    "}\n",
    "@tf.function(input_signature=[signature_dict])\n",
    "def preprocessing_fn(inputs):\n",
    "    album = inputs[\"album\"]\n",
    "    # artist = inputs[\"artist\"]\n",
    "    # track = inputs[\"track\"]\n",
    "\n",
    "    # tables = {}\n",
    "\n",
    "    with tf.init_scope():\n",
    "        src = f\"/tmp/vocab/lookup/album/\"\n",
    "        lookup_layer = tf.keras.layers.StringLookup()\n",
    "        lookup_layer.load_assets(src)\n",
    "    # lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "    #     tf.lookup.KeyValueTensorInitializer(\n",
    "    #         keys=tf.constant([\"a\", \"b\", \"c\"]),\n",
    "    #         values=tf.cast(tf.range(len([\"a\", \"b\", \"c\"])), dtype=tf.int64),\n",
    "    #         key_dtype=tf.string,\n",
    "    #         value_dtype=tf.int64,\n",
    "    #     ),\n",
    "    #     num_oov_buckets=1,\n",
    "    # )\n",
    "        # src = f\"/tmp/vocab/lookup/album/\"\n",
    "        # lookup_table = create_file_lookup(src)\n",
    "        # initializer = tf.lookup.TextFileInitializer(\n",
    "        #     src,\n",
    "        #     key_dtype=tf.string, \n",
    "        #     key_index=tf.lookup.TextFileIndex.WHOLE_LINE, \n",
    "        #     value_dtype=tf.int64, \n",
    "        #     value_index=tf.lookup.TextFileIndex.LINE_NUMBER,\n",
    "        #     value_index_offset=1, # starting from 1\n",
    "        # )\n",
    "        # lookup_table = tf.lookup.StaticHashTable(initializer, 0)\n",
    "\n",
    "        # lookup_table = tf.keras.layers.StringLookup()\n",
    "        # lookup_table.load_assets(\"/tmp/vocab/lookup/album/\")\n",
    "        # resolve issue with uninitialized variables\n",
    "        # for lookup_name in [\"album\", \"artist\", \"track\"]:\n",
    "\n",
    "            \n",
    "        #     lookup_layer = tf.keras.layers.StringLookup()\n",
    "        #     lookup_layer.load_assets(src)\n",
    "        #     vocab = lookup_layer.get_vocabulary()\n",
    "\n",
    "        #     # vocab = get_vocab(f\"/tmp/vocab/lookup/{lookup_name}/\")\n",
    "\n",
    "        #     lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "        #         tf.lookup.KeyValueTensorInitializer(\n",
    "        #             keys=tf.constant(vocab),\n",
    "        #             values=tf.cast(tf.range(len(vocab)), dtype=tf.int64),\n",
    "        #             key_dtype=tf.string,\n",
    "        #             value_dtype=tf.int64,\n",
    "        #         ),\n",
    "        #         num_oov_buckets=1,\n",
    "        #     )\n",
    "\n",
    "            # tables[lookup_name] = get_lookup_table(f\"/tmp/vocab/lookup/{lookup_name}/\")\n",
    "    out = lookup_layer(album)\n",
    "    out = [out] * 3\n",
    "\n",
    "    return out\n",
    "    \n",
    "    # return [\n",
    "    #     lookup_table.lookup(album),\n",
    "    #     # tables[\"artist\"].lookup(artist),\n",
    "    #     # tables[\"track\"].lookup(track),\n",
    "    # ] * 3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(tf.Module):\n",
    "\n",
    "  def __init__(self, src):\n",
    "    self.lookup = tf.lookup.StaticVocabularyTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(\n",
    "            keys=tf.constant([\"a\", \"b\", \"c\"]),\n",
    "            values=tf.cast(tf.range(len([\"a\", \"b\", \"c\"])), dtype=tf.int64),\n",
    "            key_dtype=tf.string,\n",
    "            value_dtype=tf.int64,\n",
    "        ),\n",
    "        num_oov_buckets=1,\n",
    "    )\n",
    "  def __call__(self, x):\n",
    "    return self.lookup.lookup(x)\n",
    "\n",
    "    \n",
    "\n",
    "model = Preprocessing(\"/tmp/vocab/lookup/album/\")\n",
    "model(tf.constant([\"a\", \"b\", \"c\"]))\n",
    "\n",
    "\n",
    "signature_dict = {\n",
    "\"album\": tf.TensorSpec(shape=(2), dtype=tf.string), \n",
    "\"artist\": tf.TensorSpec(shape=(2), dtype=tf.string),\n",
    "\"track\": tf.TensorSpec(shape=(2), dtype=tf.string)\n",
    "}\n",
    "@tf.function(input_signature=[signature_dict])\n",
    "def example1_preprocess(inputs):  # Optional: preprocessor in TF.\n",
    "  album = inputs[\"album\"]\n",
    "  out = model(album)\n",
    "  # with tf.init_scope():\n",
    "  #   src = f\"/tmp/vocab/lookup/album/\"\n",
    "  #   lookup_layer = tf.keras.layers.StringLookup()\n",
    "  #   lookup_layer.load_assets(src)\n",
    "  # norm_inputs = tf.nest.map_structure(lambda x: lookup(x), album)\n",
    "  return [out] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 2])>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 2])>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 2])>]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_fn({\"album\": tf.constant([\"a\", \"b\"]), \"artist\": tf.constant([\"a\", \"b\"]), \"track\": tf.constant([\"a\", \"b\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(name='lookup')\n",
    "class Lookup(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_lookup_layer, **kwargs):\n",
    "        super(Lookup, self).__init__(**kwargs)\n",
    "        # save the constructor parameters for get_config() to work properly\n",
    "        self.vocab_lookup_layer = vocab_lookup_layer\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        # split the string on spaces, and make it a rectangular tensor\n",
    "        inputs = x[\"album\"]\n",
    "        tokens = self.vocab_lookup_layer.lookup(inputs)\n",
    "        return [tokens] * 3\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        # save constructor args\n",
    "        config['vocab_lookup_layer'] = self.vocab_lookup_layer\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.lookup.StaticVocabularyTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(\n",
    "            keys=tf.constant([\"a\", \"b\", \"c\"]),\n",
    "            values=tf.cast(tf.range(len([\"a\", \"b\", \"c\"])), dtype=tf.int64),\n",
    "            key_dtype=tf.string,\n",
    "            value_dtype=tf.int64,\n",
    "        ),\n",
    "        num_oov_buckets=1,\n",
    "    )\n",
    "l = Lookup(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "signature_dict = {\n",
    "\"album\": tf.TensorSpec(shape=(2), dtype=tf.string), \n",
    "\"artist\": tf.TensorSpec(shape=(2), dtype=tf.string),\n",
    "\"track\": tf.TensorSpec(shape=(2), dtype=tf.string)\n",
    "}\n",
    "@tf.function(input_signature=[signature_dict])\n",
    "def example1_preprocess(inputs):  # Optional: preprocessor in TF.\n",
    "  album = inputs[\"album\"]\n",
    "  # out = model(album)\n",
    "  with tf.init_scope():\n",
    "    t = tf.lookup.StaticVocabularyTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(\n",
    "            keys=tf.constant([\"a\", \"b\", \"c\"]),\n",
    "            values=tf.cast(tf.range(len([\"a\", \"b\", \"c\"])), dtype=tf.int64),\n",
    "            key_dtype=tf.string,\n",
    "            value_dtype=tf.int64,\n",
    "        ),\n",
    "        num_oov_buckets=1,\n",
    "    )\n",
    "    l = Lookup(t)\n",
    "  out = tf.cast(l({\"album\": album}), tf.int32)\n",
    "  #   src = f\"/tmp/vocab/lookup/album/\"\n",
    "  #   lookup_layer = tf.keras.layers.StringLookup()\n",
    "  #   lookup_layer.load_assets(src)\n",
    "  # norm_inputs = tf.nest.map_structure(lambda x: lookup(x), album)\n",
    "  return [out] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = tf.lookup.StaticVocabularyTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(\n",
    "                keys=tf.constant([\"a\", \"b\", \"c\"]),\n",
    "                values=tf.cast(tf.range(len([\"a\", \"b\", \"c\"])), dtype=tf.int64),\n",
    "                key_dtype=tf.string,\n",
    "                value_dtype=tf.int64,\n",
    "            ),\n",
    "            num_oov_buckets=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int64, numpy=array([0, 1, 2])>"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup.lookup(tf.constant([\"a\", \"b\", \"c\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(tf.Module):\n",
    "    def __init__(self):\n",
    "        self.lookup = tf.lookup.StaticVocabularyTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(\n",
    "                keys=tf.constant([\"a\", \"b\", \"c\"]),\n",
    "                values=tf.cast(tf.range(len([\"a\", \"b\", \"c\"])), dtype=tf.int64),\n",
    "                key_dtype=tf.string,\n",
    "                value_dtype=tf.int64,\n",
    "            ),\n",
    "            num_oov_buckets=1,\n",
    "        )\n",
    "    \n",
    "    @tf.function(input_signature=[signature_dict])\n",
    "    def __call__(self, inputs):\n",
    "        album = inputs[\"album\"]\n",
    "        out = tf.cast(self.lookup.lookup(album), tf.int32)\n",
    "        return [out] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.lookup.StaticVocabularyTable(\n",
    "      tf.lookup.KeyValueTensorInitializer(\n",
    "          keys=tf.constant([\"a\", \"b\", \"c\"]),\n",
    "          values=tf.cast(tf.range(len([\"a\", \"b\", \"c\"])), dtype=tf.int64),\n",
    "          key_dtype=tf.string,\n",
    "          value_dtype=tf.int64,\n",
    "      ),\n",
    "      num_oov_buckets=1,\n",
    "  )\n",
    "\n",
    "m = tf.Module()\n",
    "m.t = t\n",
    "\n",
    "signature_dict = {\n",
    "\"album\": tf.TensorSpec(shape=(2), dtype=tf.string), \n",
    "\"artist\": tf.TensorSpec(shape=(2), dtype=tf.string),\n",
    "\"track\": tf.TensorSpec(shape=(2), dtype=tf.string)\n",
    "}\n",
    "@tf.function(input_signature=[signature_dict])\n",
    "def example1_preprocess(inputs):  # Optional: preprocessor in TF.\n",
    "  album = inputs[\"album\"]\n",
    "  # out = model(album)\n",
    "  \n",
    "  out = tf.cast(m.t.lookup(album), tf.int32)\n",
    "  #   src = f\"/tmp/vocab/lookup/album/\"\n",
    "  #   lookup_layer = tf.keras.layers.StringLookup()\n",
    "  #   lookup_layer.load_assets(src)\n",
    "  # norm_inputs = tf.nest.map_structure(lambda x: lookup(x), album)\n",
    "  return [out] * 3\n",
    "\n",
    "m.serving = example1_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tried to export a function which references an 'untracked' resource. TensorFlow objects (e.g. tf.Variable) captured by functions must be 'tracked' by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly. See the information below:\n\tFunction name = b'__inference_signature_wrapper_inference_fn_20160'\n\tCaptured Tensor = <ResourceHandle(name=\"19975\", device=\"/job:localhost/replica:0/task:0/device:CPU:0\", container=\"localhost\", type=\"tensorflow::lookup::LookupInterface\", dtype and shapes : \"[  ]\")>\n\tTrackable referencing this tensor = <tensorflow.python.ops.lookup_ops.HashTable object at 0x151723820>\n\tInternal Tensor = Tensor(\"20134:0\", shape=(), dtype=resource)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/stefruinard/Documents/work/202311_ml_for_llms/flax-mlops/interactive.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stefruinard/Documents/work/202311_ml_for_llms/flax-mlops/interactive.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m export_mgr \u001b[39m=\u001b[39m ExportManager(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stefruinard/Documents/work/202311_ml_for_llms/flax-mlops/interactive.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   jax_module, [\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stefruinard/Documents/work/202311_ml_for_llms/flax-mlops/interactive.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     ServingConfig(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stefruinard/Documents/work/202311_ml_for_llms/flax-mlops/interactive.ipynb#X34sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stefruinard/Documents/work/202311_ml_for_llms/flax-mlops/interactive.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m ])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stefruinard/Documents/work/202311_ml_for_llms/flax-mlops/interactive.ipynb#X34sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/tmp/example1_output_dir\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stefruinard/Documents/work/202311_ml_for_llms/flax-mlops/interactive.ipynb#X34sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m export_mgr\u001b[39m.\u001b[39;49msave(output_dir)\n",
      "File \u001b[0;32m~/Documents/work/202311_ml_for_llms/flax-mlops/venv/lib/python3.9/site-packages/orbax/export/export_manager.py:109\u001b[0m, in \u001b[0;36mExportManager.save\u001b[0;34m(self, model_path, save_options, signature_overrides)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m signature_overrides:\n\u001b[1;32m    107\u001b[0m   serving_signatures\u001b[39m.\u001b[39mupdate(signature_overrides)\n\u001b[0;32m--> 109\u001b[0m tf\u001b[39m.\u001b[39;49msaved_model\u001b[39m.\u001b[39;49msave(\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtf_module, model_path, serving_signatures, options\u001b[39m=\u001b[39;49msave_options\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m get_current_dtensor_mesh():\n\u001b[1;32m    114\u001b[0m   \u001b[39m# TODO(b/261191533): we can remove this once tf.saved_model.save is aware\u001b[39;00m\n\u001b[1;32m    115\u001b[0m   \u001b[39m# of SPMD saving.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m   dtensor\u001b[39m.\u001b[39mbarrier(get_current_dtensor_mesh(), \u001b[39m'\u001b[39m\u001b[39mexport done\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/work/202311_ml_for_llms/flax-mlops/venv/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:1331\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[39m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m metrics\u001b[39m.\u001b[39mIncrementWriteApi(_SAVE_V2_LABEL)\n\u001b[0;32m-> 1331\u001b[0m save_and_return_nodes(obj, export_dir, signatures, options)\n\u001b[1;32m   1333\u001b[0m metrics\u001b[39m.\u001b[39mIncrementWrite(write_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m2\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/work/202311_ml_for_llms/flax-mlops/venv/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:1366\u001b[0m, in \u001b[0;36msave_and_return_nodes\u001b[0;34m(obj, export_dir, signatures, options, experimental_skip_checkpoint)\u001b[0m\n\u001b[1;32m   1362\u001b[0m saved_model \u001b[39m=\u001b[39m saved_model_pb2\u001b[39m.\u001b[39mSavedModel()\n\u001b[1;32m   1363\u001b[0m meta_graph_def \u001b[39m=\u001b[39m saved_model\u001b[39m.\u001b[39mmeta_graphs\u001b[39m.\u001b[39madd()\n\u001b[1;32m   1365\u001b[0m _, exported_graph, object_saver, asset_info, saved_nodes, node_paths \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1366\u001b[0m     _build_meta_graph(obj, signatures, options, meta_graph_def))\n\u001b[1;32m   1367\u001b[0m saved_model\u001b[39m.\u001b[39msaved_model_schema_version \u001b[39m=\u001b[39m (\n\u001b[1;32m   1368\u001b[0m     constants\u001b[39m.\u001b[39mSAVED_MODEL_SCHEMA_VERSION)\n\u001b[1;32m   1370\u001b[0m \u001b[39m# Write the checkpoint, copy assets into the assets directory, and write out\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m \u001b[39m# the SavedModel proto itself.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/work/202311_ml_for_llms/flax-mlops/venv/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:1578\u001b[0m, in \u001b[0;36m_build_meta_graph\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a MetaGraph under a save context.\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \n\u001b[1;32m   1553\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1574\u001b[0m \u001b[39m  saveable_view.node_paths: _SaveableView paths.\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1577\u001b[0m \u001b[39mwith\u001b[39;00m save_context\u001b[39m.\u001b[39msave_context(options):\n\u001b[0;32m-> 1578\u001b[0m   \u001b[39mreturn\u001b[39;00m _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\n",
      "File \u001b[0;32m~/Documents/work/202311_ml_for_llms/flax-mlops/venv/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:1504\u001b[0m, in \u001b[0;36m_build_meta_graph_impl\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1502\u001b[0m saveable_view \u001b[39m=\u001b[39m _SaveableView(augmented_graph_view, options)\n\u001b[1;32m   1503\u001b[0m object_saver \u001b[39m=\u001b[39m checkpoint\u001b[39m.\u001b[39mTrackableSaver(augmented_graph_view)\n\u001b[0;32m-> 1504\u001b[0m asset_info, exported_graph \u001b[39m=\u001b[39m _fill_meta_graph_def(\n\u001b[1;32m   1505\u001b[0m     meta_graph_def,\n\u001b[1;32m   1506\u001b[0m     saveable_view,\n\u001b[1;32m   1507\u001b[0m     signatures,\n\u001b[1;32m   1508\u001b[0m     options\u001b[39m.\u001b[39;49mnamespace_whitelist,\n\u001b[1;32m   1509\u001b[0m     options\u001b[39m.\u001b[39;49mexperimental_custom_gradients,\n\u001b[1;32m   1510\u001b[0m     defaults,\n\u001b[1;32m   1511\u001b[0m )\n\u001b[1;32m   1512\u001b[0m \u001b[39mif\u001b[39;00m options\u001b[39m.\u001b[39mfunction_aliases:\n\u001b[1;32m   1513\u001b[0m   function_aliases \u001b[39m=\u001b[39m meta_graph_def\u001b[39m.\u001b[39mmeta_info_def\u001b[39m.\u001b[39mfunction_aliases\n",
      "File \u001b[0;32m~/Documents/work/202311_ml_for_llms/flax-mlops/venv/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:877\u001b[0m, in \u001b[0;36m_fill_meta_graph_def\u001b[0;34m(meta_graph_def, saveable_view, signature_functions, namespace_whitelist, save_custom_gradients, defaults)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mwith\u001b[39;00m exported_graph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m    876\u001b[0m   object_map, tensor_map, asset_info \u001b[39m=\u001b[39m saveable_view\u001b[39m.\u001b[39mmap_resources()\n\u001b[0;32m--> 877\u001b[0m   signatures \u001b[39m=\u001b[39m _generate_signatures(signature_functions, object_map, defaults)\n\u001b[1;32m    878\u001b[0m \u001b[39mif\u001b[39;00m save_custom_gradients:\n\u001b[1;32m    879\u001b[0m   \u001b[39m# Custom gradients functions must be traced in the same context as the\u001b[39;00m\n\u001b[1;32m    880\u001b[0m   \u001b[39m# when they are registered.\u001b[39;00m\n\u001b[1;32m    881\u001b[0m   _trace_gradient_functions(exported_graph, saveable_view)\n",
      "File \u001b[0;32m~/Documents/work/202311_ml_for_llms/flax-mlops/venv/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:654\u001b[0m, in \u001b[0;36m_generate_signatures\u001b[0;34m(signature_functions, object_map, defaults)\u001b[0m\n\u001b[1;32m    646\u001b[0m   mapped_inputs, exterior_argument_placeholders \u001b[39m=\u001b[39m (\n\u001b[1;32m    647\u001b[0m       _map_function_arguments_to_created_inputs(\n\u001b[1;32m    648\u001b[0m           argument_inputs, signature_key, function\u001b[39m.\u001b[39mname, defaults\n\u001b[1;32m    649\u001b[0m       )\n\u001b[1;32m    650\u001b[0m   )\n\u001b[1;32m    651\u001b[0m   kwarg_names \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m    652\u001b[0m       \u001b[39msorted\u001b[39m(\n\u001b[1;32m    653\u001b[0m           object_map[function]\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39mstructured_input_signature[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mkeys()))\n\u001b[0;32m--> 654\u001b[0m   outputs \u001b[39m=\u001b[39m object_map[function](\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\n\u001b[1;32m    655\u001b[0m       kwarg_name: mapped_input\n\u001b[1;32m    656\u001b[0m       \u001b[39mfor\u001b[39;49;00m kwarg_name, mapped_input \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(kwarg_names, mapped_inputs)\n\u001b[1;32m    657\u001b[0m   })\n\u001b[1;32m    658\u001b[0m   signatures[signature_key] \u001b[39m=\u001b[39m signature_def_utils\u001b[39m.\u001b[39mbuild_signature_def(\n\u001b[1;32m    659\u001b[0m       _tensor_dict_to_tensorinfo(exterior_argument_placeholders),\n\u001b[1;32m    660\u001b[0m       _tensor_dict_to_tensorinfo(outputs),\n\u001b[1;32m    661\u001b[0m       method_name\u001b[39m=\u001b[39msignature_constants\u001b[39m.\u001b[39mPREDICT_METHOD_NAME,\n\u001b[1;32m    662\u001b[0m       defaults\u001b[39m=\u001b[39mdefaults\u001b[39m.\u001b[39mget(signature_key, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    663\u001b[0m   )\n\u001b[1;32m    664\u001b[0m \u001b[39mreturn\u001b[39;00m signatures\n",
      "File \u001b[0;32m~/Documents/work/202311_ml_for_llms/flax-mlops/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/saved_model_exported_concrete.py:45\u001b[0m, in \u001b[0;36mExportedConcreteFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m bound_arguments \u001b[39m=\u001b[39m function_type_utils\u001b[39m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m     40\u001b[0m     args, kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39m_function_type\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m filtered_flat_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39m_function_type\u001b[39m.\u001b[39munpack_inputs(\n\u001b[1;32m     43\u001b[0m     bound_arguments\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 45\u001b[0m export_captures \u001b[39m=\u001b[39m _map_captures_to_created_tensors(\n\u001b[1;32m     46\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction\u001b[39m.\u001b[39;49mgraph\u001b[39m.\u001b[39;49mcaptures, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensor_map, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction)\n\u001b[1;32m     47\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39m_call_flat(filtered_flat_args, export_captures)\n",
      "File \u001b[0;32m~/Documents/work/202311_ml_for_llms/flax-mlops/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/saved_model_exported_concrete.py:74\u001b[0m, in \u001b[0;36m_map_captures_to_created_tensors\u001b[0;34m(original_captures, tensor_map, function)\u001b[0m\n\u001b[1;32m     72\u001b[0m   mapped_resource \u001b[39m=\u001b[39m tensor_map\u001b[39m.\u001b[39mget(exterior, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     73\u001b[0m   \u001b[39mif\u001b[39;00m mapped_resource \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     _raise_untracked_capture_error(function\u001b[39m.\u001b[39;49mname, exterior, interior)\n\u001b[1;32m     75\u001b[0m   export_captures\u001b[39m.\u001b[39mappend(mapped_resource)\n\u001b[1;32m     76\u001b[0m \u001b[39mreturn\u001b[39;00m export_captures\n",
      "File \u001b[0;32m~/Documents/work/202311_ml_for_llms/flax-mlops/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/saved_model_exported_concrete.py:98\u001b[0m, in \u001b[0;36m_raise_untracked_capture_error\u001b[0;34m(function_name, capture, internal_capture, node_path)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m internal_capture \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m   msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mInternal Tensor = \u001b[39m\u001b[39m{\u001b[39;00minternal_capture\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tried to export a function which references an 'untracked' resource. TensorFlow objects (e.g. tf.Variable) captured by functions must be 'tracked' by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly. See the information below:\n\tFunction name = b'__inference_signature_wrapper_inference_fn_20160'\n\tCaptured Tensor = <ResourceHandle(name=\"19975\", device=\"/job:localhost/replica:0/task:0/device:CPU:0\", container=\"localhost\", type=\"tensorflow::lookup::LookupInterface\", dtype and shapes : \"[  ]\")>\n\tTrackable referencing this tensor = <tensorflow.python.ops.lookup_ops.HashTable object at 0x151723820>\n\tInternal Tensor = Tensor(\"20134:0\", shape=(), dtype=resource)"
     ]
    }
   ],
   "source": [
    "jax_module = JaxModule({\"params\": state.params}, state.apply_fn)\n",
    "# Export the JaxModule along with one or more serving configs.\n",
    "export_mgr = ExportManager(\n",
    "  jax_module, [\n",
    "    ServingConfig(\n",
    "      'serving_default',\n",
    "      # input_signature=[\n",
    "      #   {\n",
    "      #     \"album\": tf.TensorSpec(shape=(2), dtype=tf.string), \n",
    "      #     \"artist\": tf.TensorSpec(shape=(2), dtype=tf.string),\n",
    "      #     \"track\": tf.TensorSpec(shape=(2), dtype=tf.string)\n",
    "      #   }\n",
    "      # ],\n",
    "      tf_preprocessor=m.serving,\n",
    "      # tf_postprocessor=example1_postprocess\n",
    "    ),\n",
    "])\n",
    "output_dir='/tmp/example1_output_dir'\n",
    "export_mgr.save(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
